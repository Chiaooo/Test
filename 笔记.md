1.项目概况

2.理论基础与实践

3.结果展示

4.结果分析与总结

5.参考文献


# 1.项目概况

## 1.1 项目名称

新冠肺炎疫情实时监控。项目地址：http://121.41.228.239:5000

## 1.2 项目介绍

疫情在家期间独立开发此项目。该项目是一个基于 **Python + Flask + Echarts** 打造的一个全球疫情监控系统，它能够实时监控并统计**中国**以及**全世界各国**新冠肺炎的**确诊人数**，同时**以图表和地图的形式**展现出来。项目完成的功能如下：

- 统计全国各省市地区每日疫情数据，并以图表形式展现。
- 统计全国疫情历史数据，并以图表形式展现。
- 统计百度热搜数据，并以词云图形式展现。
- 统计全球各国疫情数据。

项目涉及的主要技术栈有：

- Python网络爬虫
- Python与MySQL数据库交互
- Flask构建Web项目
- 基于Echarts数据可视化展示
- 在阿里云部署Web项目以及爬虫

## 1.3 项目流程图

<img src="https://img2020.cnblogs.com/blog/1179840/202008/1179840-20200804192426803-1392761535.png" style="zoom:80%;" />

# 2.理论基础与实践

## 2.1 腾讯疫情数据爬取

有一定爬虫基础的同学，可以尝试去全国各地的卫健委网站上爬取各省市地区的疫情数据，不过部分卫健委网站反爬虫手段很高明，需要专业的反反爬手段，**所以本文采用直接去各大疫情报告平台直接爬取最终的疫情数据**，例如**腾讯（本项目数据来源）**、百度、丁香园。

### 2.1.1 爬虫概述

爬虫是一个给网站发起请求，并从网站的响应中提取需要的数据的自动化程序。

爬虫的基本流程如下：

1. **向网站发起请求，获取网站的响应**

   通过一些常用库（urllib、urllib3、**requests**）等对目标站点进行请求，等同于自己打开浏览器，输入网址访问。服务器返回的响应内容的格式通常为：html、二进制文件（视频、音频）、文档、Json字符串等。

2. **解析网站响应的内容(数据清洗)**

   寻找自己需要的内容，就是利用正则表达式或者其他库提取目标信息

   常用库（re、beautifulsoup4）

3. **保存数据至数据库中**

   将解析得到的数据持久化到文件或者数据库中。

### 2.1.2 爬取腾讯疫情数据

利用谷歌浏览器开发者选项分析腾讯疫情数据网站，经过解析，我们发现网站上所有疫情的数据都来源于以下几个url对应的api接口，由于网站提供了数据的 api 接口，利用*postman*工具测试可得返回的数据为 json 格式，所以我们只需要调用 json 库中相应的方法就可以完成数据解析。

```http
https://view.inews.qq.com/g2/getOnsInfo?name=disease_h5			# 全国各省市地区每日疫情数据
https://view.inews.qq.com/g2/getOnsInfo?name=disease_other		# 全国疫情历史数据
https://api.inews.qq.com/newsqa/v1/automation/foreign/daily/list?country=国家名字	# 国外各国疫情数据
```

全国各省市地区每日疫情数据

<img src="https://img2020.cnblogs.com/blog/1179840/202103/1179840-20210316143836406-904414921.png" style="zoom:80%;" />

全国疫情历史数据

<img src="https://img2020.cnblogs.com/blog/1179840/202103/1179840-20210316143908150-495379628.png" style="zoom:80%;" />

世界各国疫情历史数据（以加拿大为例）

<img src="https://img2020.cnblogs.com/blog/1179840/202103/1179840-20210316143935564-1800353752.png" style="zoom:80%;" />

### 2.1.3 数据解析

从2.1.2中分析可得，虽然我们成功得到了目标网址的响应，但是得到的内容第一眼看上去是杂乱无章的，所以我们需要对网址响应进行解析，从中提取我们需要的数据，同时也可以根据这些数据字段完成数据库表的构建。解析数据的基本原理如下：

- 我们可以发现API返回的数据格式为Json格式，所以我们可以利用python第三方库将Json格式转化为python中我们熟悉的字典格式，然后利用熟悉的字典键值对相关操作获取我们想要的数据，整个过程比较繁琐，只要是耐心一点最终是可以获取到我们想要的数据，此时爬取的数据目前存在内存中，我们必须对这么数据进行持久化操作。

## 2.2 百度热搜数据爬取

如果还利用常规的爬虫方法对百度疫情热搜页面进行爬取，我们会发现爬取的结果如下

<img src="https://img2020.cnblogs.com/blog/1179840/202008/1179840-20200821185414069-35474619.png" style="zoom:80%;" />

对百度热搜数据进行分析之后可以发现**百度的数据页面使用了`JS动态渲染技术`**（换句话说：这些数据是网站动态加载的），所以我们用普通的爬虫方法是行不通的，所以我们可以采用 `selenium` 工具来爬取。

### 2.2.1 selenium介绍

`Selenium`是一个用于Web应用程序测试的工具。**Selenium测试直接运行在浏览器中，就像真正的用户在操作一样**。要想利用该工具，还需要你安装对应的浏览器（谷歌、火狐）和[浏览器驱动](http://npm.taobao.org/mirrors/chromedriver/)。

`Selenium`使用的三个步骤：

- 创建浏览器对象 
- 浏览器.get() 
- 浏览器.find()

### 2.2.2 爬取结果

<img src="https://img2020.cnblogs.com/blog/1179840/202008/1179840-20200821194145567-1828590351.png" style="zoom:67%;" />

与爬取腾讯疫情数据一样，我们对这些爬取到的数据结果一样要进行解析以及持久化操作，解析和持久化过程与处理腾讯疫情数据原理同上。

## 2.3 世界各国疫情数据爬取

前文分析到，世界各国疫情数据的api接口为https://api.inews.qq.com/newsqa/v1/automation/foreign/daily/list?country= + 国家名字，所以要爬取世界各国疫情数据也比较容易，具体步骤如下：

1. 将世界各国国家名字存在数组中
2. 遍历数组，将国家名字拼接至api接口
3. 对爬取的数据进行解析，并持久化至数据库中。

至此，完成了数据爬取工作，接下来需要对保存在内存中的数据进行持久化操作。

## 2.4 Python与MySQL数据库交互

### 2.4.1 连接数据库

使用 `pymysql` 第三方模块与数据库交互 ,数据库交互简单来说就是4个步骤：

- 建立连接 
- 创建游标 
- 根据游标执行操作 
- 关闭连接

```python
def get_conn():
	# 建立连接，可以连接本地数据库或者直接连接服务器的数据库
	conn = pymysql.connect(host = "127.0.0.1", user= "root", password = "123456", db ="cov",charset="utf8")
	# 创建游标
	cursor = conn.cursor()
	return conn, cursor

def close_conn(conn, cursor):
	if cursor:
		cursor.close()
	if conn:
		conn.close()
```

### 2.2.2 数据库建表

根据爬取的**腾讯疫情数据**、**百度热搜数据**以及**世界各国疫情数据**，我们需要在数据库中建立四张表格，分别名为**details(各省市疫情数据)、fforeign(世界各国疫情数据)、history(中国疫情历史数据)、hotsearch(热搜数据)**，数据库中四张表格如下：

<img src="https://img2020.cnblogs.com/blog/1179840/202012/1179840-20201224084654490-1379429589.png" style="zoom:80%;" />

details表字段如下：

<img src="https://img2020.cnblogs.com/blog/1179840/202012/1179840-20201224084745026-1556647380.png" style="zoom:80%;" />

fforeign表字段如下:

<img src="C:\Users\汤国频\AppData\Roaming\Typora\typora-user-images\image-20201224084822834.png" alt="image-20201224084822834" style="zoom:80%;" />

history表字段如下:

<img src="https://img2020.cnblogs.com/blog/1179840/202012/1179840-20201224084843209-1551825580.png" style="zoom:80%;" />

hotsearch表字段如下:

<img src="https://img2020.cnblogs.com/blog/1179840/202012/1179840-20201224084843209-1551825580.png" style="zoom:80%;" />

### 2.2.3 数据库交互

数据持久化过程中最难是建表，在建表的过程中需要考虑主键以及其他属性等等，而与数据库交互的过程就是CURD逻辑代码的书写。唯一要注意的就是**数据库中时间字段的类型与python中时间字段类型是不一致的，所以在与数据库交互之前，必须进行类型的强制转换**。

## 2.5 Flask构建Web项目

### 2.5.1 Flask介绍

- Flask是一个基于Python语言且**遵守WSGI协议**的轻量级Web开发框架，非常适合小项目的编写，只需要具备基本的`python`开发技能，就可以开发出一个web应用来。
- 相比于`Django`, `Flask`算的上是微型的Web框架了,它只有**路由**和**模板渲染**两个功能, 想干别的事都需要使用插件.
- 和Django大包大揽不同，Flask建立于一系列的开源软件包之上，这其中最主要的是WSGI应用开发库[Werkzeug](https://www.xncoding.com/2016/12/02/python/werkzeug.html#:~:text=Werkzeug%E6%98%AF%E4%B8%80%E4%B8%AA%E4%B8%93%E9%97%A8%E7%94%A8,%E5%A4%84%E7%90%86HTTP%E5%8D%8F%E8%AE%AE%E7%9B%B8%E5%85%B3%E5%86%85%E5%AE%B9%E3%80%82&text=%E4%BE%8B%E5%A6%82%E6%88%91%E6%9C%80%E5%B8%B8%E7%94%A8%E7%9A%84,%E5%AE%9E%E7%8E%B0%E9%80%BB%E8%BE%91%E4%BB%A5%E5%8F%8A%E5%BA%95%E5%B1%82%E6%8E%A7%E5%88%B6%E3%80%82)和模板引擎[Jinja2](https://wsgzao.github.io/post/jinja/)：

- 主要特点小而轻，属于短小精悍型框架，同时入门简单，通过官方指南便可以清楚的了解Flask的运行流程 [Flask快速上手](https://dormousehole.readthedocs.io/en/latest/quickstart.html#quickstart)；

### 2.5.2 Flask简单入门

```python
# 首先我们导入 Flask 类。这个类的实例将会成为我们的WSGI应用。
from flask import Flask

# 接着我们创建这个类的实例。第一个参数是应用模块或者包的名称。如果你使用一个单一模块（就像本例），那么应当使用 __name__ ，因为名称会根据这个模块是按应用方式使用还是作为一个模块导入而发生变化（可能是'__main__' ，也可能是实际导入的名称）。这个参数是必需的，这样 Flask就可以知道在哪里找到模板和静态文件等东西。更多内容详见Flask文档。
app = Flask(__name__)

# 然后我们使用#route()装饰器来告诉 Flask 触发函数的 URL，这里绑定到了根目录，意思就是说当输入IP和PROT之后就可以访问hello_world函数 。
@app.route('/')

# 函数名称可用于生成相关联的 URL ，并返回需要在用户浏览器中显示的信息。
def hello_world():
    return 'Hello World!'

# 最后，使用 run() 函数来运行本地服务器和我们的应用。if__name__== '__main__': 确保服务器只会在使用 Python 解释器运行代码的情况下运行，而不会在作为模块导入时运行。
if __name__ == '__main__':
    app.run()
```

### 2.5.3 前端布局

最终达到的页面布局的效果如下图所示

<img src="https://img2020.cnblogs.com/blog/1179840/202008/1179840-20200830132930503-1917694484.png" style="zoom:80%;" />

前端页面布局主要利用CSS语言进行编写，根据下面绘制的简单页面规划图以及基本的CSS语句即可完成前端页面布局的书写。

<img src="https://img2020.cnblogs.com/blog/1179840/202008/1179840-20200830140441800-1646920681.png" style="zoom: 50%;" />

基于以上原理，最终完成的效果图如下：

<img src="https://img2020.cnblogs.com/blog/1179840/202008/1179840-20200830141221093-1320153110.png" style="zoom: 67%;" />

与效果图相比，我们还少了4个方框和中间一个中国地图以及一个世界地图！而图表的展示就要借用Echarts工具。

## 2.6 基于Echarts图表展示

### 2.6.1 Echarts原理介绍

`ECharts`，缩写来自`Enterprise Charts`，商业级数据图表，是百度的一个开源的数据可视化工具，提供了丰富的图表库，能够在PC端和移动设备上流畅运行。在前端代码中引入Echarts图表是非常简单的，引入Echarts图表的代码如下：

```html
<!DOCTYPE html>
<html>
<head>
    <meta charset="utf-8">
    <!-- 引入 ECharts 文件 -->
    <script src="echarts.min.js"></script>
</head>
</html>
```

### 2.6.2 插入中国地图

分析可得，目前我们需要的图表一共有六个：**词云图、中国地图、世界地图、3个柱状图或者折线图。**这些图表的js文件在网上都有相应的资源，下载后引入前端界面即可。

以中国地图为例：

- 导入`china.js`（网上可以下载） 
- 复制中国地图option，加以整理至`ec_center.js`文件中。

在前端html文件中引入相应的文件，内容如下：

<img src="https://img2020.cnblogs.com/blog/1179840/202008/1179840-20200830150121491-160535834.png" style="zoom:80%;" />

**最终效果图如下：**

<img src="https://img2020.cnblogs.com/blog/1179840/202008/1179840-20200830145700677-1220835986.png" style="zoom:80%;" />

**剩余几个图表的操作过程和原理与引入中国地图相同。**

## 2.7 前后端交互

经过上述步骤，成功得到了一个静态的前端显示页面，

<img src="https://img2020.cnblogs.com/blog/1179840/202008/1179840-20200830132930503-1917694484.png" style="zoom: 50%;" />

<img src="https://img2020.cnblogs.com/blog/1179840/202012/1179840-20201224091654315-624472013.png" style="zoom:80%;" />

以上页面与项目最终结果相比，还缺少数据的填充，即从后台数据库读取疫情数据，并在前端页面显示出来。

### 2.7.1 Ajax介绍

Ajax 是 `Asynchronous JavaScript and XML` 的简称，通过 Ajax 向服务器发送请求，接收服务器返回的 json 数据，然后使用 `JavaScript(js)` 修改网页的来实现页面局部数据更新。Ajax的基本格式如下：

<img src="https://img2020.cnblogs.com/blog/1179840/202008/1179840-20200830153215914-1112207910.png" style="zoom:80%;" />

根据Ajax原理以及前文实现的数据库相关操作，既可以完成前端页面数据的填写。

## 2.8 在阿里云部署Web项目以及爬虫

将项目部署到云服务器上，首先需要有一台私人服务器，可以选择腾讯云或者阿里云。如何购买并初始化服务器，该内容在网上的教程很多，再次不做介绍。

然后利用实现SSH协议等软件将个人项目打包上传值个人服务器。接下来最重要的步骤就是**Nginx代理服务器**以及**gunicornweb服务器**的配置与启动。

### 2.8.1 Nginx代理配置

Nginx是异步框架的网页服务器，也可以用作反向代理、负载平衡器和HTTP缓存。

```tex
如果不用nginx一样可以访问你的项目,使用nginx的目的是为了安全和负载均衡，配置了nginx做前端代理,gunicorn作后端代理的服务器(这里所说的前后端都是相对的位置,并无实际含义),在处理来自Internet的请求时,要先经过nginx的处理,nginx把请求再交给gunicorn,经过gunicorn才能访问到项目本身，没有nginx而只有gunicorn的服务器,则是Internet请求直接由gunicorn处理,并反馈到我们的项目中，nginx可以实现安全过滤,防DDOS等保护安全的操作,并且如果配置了多台服务器,nginx可以保证服务器的负载相对均衡。
```

编辑Nginx的配置文件：`vim /etc/nginx/nginx.conf`

使用upstream配置服务地址，**使用server的location配置代理映射。**

该配置文件表达的含义是：访问服务器地址：80(端口)的请求会被转发到mycluster服务地址127.0.0.1:8080

<img src="https://img2020.cnblogs.com/blog/1179840/202009/1179840-20200901011831132-2092386506.png" style="zoom:80%;" />

### 2.8.2 gunicornWeb服务器

gunicorn是一个web服务器，就如同Tomcat一般，该服务器实现了WSGI协议(Web Server Gateway Interface)，http协议等，它可以接收和处理请求,发出响应等。

启动Gunicorn的相应命令：`gunicorn -b 127.0.0.1:8080 -D app:app`（-D：守护进程）

```tex
1.如果项目启动失败，通过命令`ps -ef | grep gunicorn`查看`gunicorn`服务器是否启动成功
2.-bash: gunicorn: command not found
  export PATH=$PATH:/usr/local/python3/bin
3.注意：必须在项目目录下启动`gunicorn`
```

# 3.结果展示

截止2020年12月24日，国内疫情官方数据如下（图片截取至腾讯疫情）：

<img src="https://img2020.cnblogs.com/blog/1179840/202012/1179840-20201224092904409-126362215.png" style="zoom:50%;" />

国外疫情官方数据（以重灾区印度为例子）:

<img src="https://img2020.cnblogs.com/blog/1179840/202012/1179840-20201224130121646-1390699265.png" style="zoom: 50%;" />

个人项目爬取的数据如下：

<img src="https://img2020.cnblogs.com/blog/1179840/202012/1179840-20201224125644676-933110886.png" style="zoom:80%;" />

印度总确诊人数

<img src="https://img2020.cnblogs.com/blog/1179840/202012/1179840-20201224130033165-376964323.png" style="zoom:80%;" />

**经过上述对比，可以发现本项目爬取并展示的数据基本正确。**

# 4.结果分析与总结

- 为了减轻数据API接口的访问压力，所以本项目的爬虫频率**一天仅两次**，因此可能会导致数据更新延迟，使得数据出现短暂的不正确。
- 在项目的完成过程中，学习了一些常见的爬虫、反爬虫、反反爬虫等常见技术，不仅开拓了视野，同时对于爬虫整个流程和原理都有了更加深刻的认识，相信以后写爬虫程序应该能够更加的得心应手。
- 项目采用主流的Mysql数据库作为数据的持久化选择，项目中设计到大量的CURD操作，需要书写大量的SQL语句，这不仅加强了我对Mysql理论知识的熟悉，更是在实践中训练了书写SQL语句的能力。
- 第一次将整个项目部署至云服务器，整个过程尤其是在服务器上调试程序的过程，是需要一定的Linux操作基础，完成整个项目，让我对于项目的分析、开发、部署整个流程更加熟悉，同时还掌握了一些Linux基本操作命令。
- 虽然说整个项目的前面页面非常简单，但是在学习的过程中，还是掌握了例如HTML\\CSS\JS等基础知识。

# 5.参考文献

[5 分钟上手 ECharts](https://echarts.apache.org/zh/tutorial.html#5%20%E5%88%86%E9%92%9F%E4%B8%8A%E6%89%8B%20ECharts)

[项目架构、wsgi、gunicore介绍](https://zhuanlan.zhihu.com/p/46983059)

[WSGI介绍](https://www.jianshu.com/p/8de20cb0fd81)

[Flask快速上手](https://dormousehole.readthedocs.io/en/latest/quickstart.html#quickstart)

[css入门](https://www.w3cschool.cn/css/)

[CentOS 下用 Nginx 和 gunicorn 部署 flask 项目](https://segmentfault.com/a/1190000004294634)
